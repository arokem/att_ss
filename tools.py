import os
import time
try: 
    import pandas as pd
    has_pd = True
except:
    has_pd = False
import numpy as np
try:
    import wx
    has_wx = True
except:
    has_wx = False
    pass

try:
    from psychopy import core, visual, event, gui
    has_psychopy = True
except:
    has_psychopy = False
    
import matplotlib
from matplotlib.mlab import window_hanning,csv2rec
import matplotlib.pyplot as plt
from scipy.optimize import leastsq
from scipy.special import erf

#User input GUI:
if has_wx:
    class GetFromGui(wx.Dialog):
        """ Allows user to set input parameters of ss through a simple GUI"""    
        def __init__(self, parent, id, title, full=True):
            self.full=full
            wx.Dialog.__init__(self, parent, id, title, size=(280,280))
            # Add text label
            wx.StaticText(self, -1, 'Subject ID:', pos=(10,20))
            # Add the subj id text box:
            self.textbox1 = wx.TextCtrl(self, -1, pos=(100, 18), size=(150, -1))

            # Add text label
            wx.StaticText(self, -1, 'Center ori:', pos=(10, 50))
            # Add the text box for the center orientation:
            self.textbox2 = wx.TextCtrl(self, -1, pos=(100, 48), size=(150, -1))

            # Add text label
            wx.StaticText(self, -1, 'Surr ori:', pos=(10, 80))
            # Add the text box for the surround orientation:
            self.textbox3 = wx.TextCtrl(self, -1, pos=(100, 78), size=(150, -1))
            if self.full:
                wx.StaticText(self, -1, "Surround with target", pos=(10,110))
                self.textbox4 = wx.TextCtrl(self, -1, pos=(150, 108),
                                            size=(100, -1))

                wx.StaticText(self, -1, "Audio feedback", pos=(10,140))
                self.textbox5 = wx.TextCtrl(self, -1, pos=(150, 138),
                                            size=(100, -1))

                wx.StaticText(self, -1, "Neutral Cue?", pos=(10,170))
                self.textbox6 = wx.TextCtrl(self, -1, pos=(150, 168),
                                            size=(100, -1))

            # Add OK/Cancel buttons
            wx.Button(self, 1, 'Done', (60, 210))
            wx.Button(self, 2, 'Quit', (150, 210))

            # Bind button press events to class methods for execution
            self.Bind(wx.EVT_BUTTON, self.OnDone, id=1)
            self.Bind(wx.EVT_BUTTON, self.OnClose, id=2)
            self.Centre()
            self.ShowModal()        

        # If "Done" is pressed, set important values and close the window
        def OnDone(self,event):

            self.success = True
            self.subject = self.textbox1.GetValue()
            center_ori = self.textbox2.GetValue()
            surr_ori = self.textbox3.GetValue()
            if self.full:
                swt = self.textbox4.GetValue()
                audio = self.textbox5.GetValue()
                cue_reliability = self.textbox6.GetValue()

            if center_ori:
                  self.center_ori = center_ori
            else:
                  self.center_ori = 0

            if surr_ori:
                self.surr_ori = surr_ori
            else:
                self.surr_ori = None

            if self.full:
                if swt:
                    self.swt = True
                else:
                    self.swt = False

                if audio:
                    self.audio = True
                else:
                    self.audio = False

                if cue_reliability:
                    self.cue_reliability = False
                else:
                    self.cue_reliability = False

            self.Close()

        # If "Quit" is pressed" , toggle failure and close the window
        def OnClose(self, event):
            self.success = False
            self.Close()


class Params(object):
    """
    The Params class stores all of the parameters needed during the execution
    of ss_run.

    Once a parameter is set, it is protected and cannot be changed, unless it
    is explicitely removed from the _dont_touch variable. 

    Some parameters are set upon initialization from the file 'ss_params.py'

    Others are set through a gui which is generated by the method set_by_gui
    
    """
    def __init__(self, p_file='params'):
        """
        Initializer for the params object.

        Parameters
        ----------

        p_file: string, the name of a parameter file, defaults to 'ss_params'

        """
        self._dont_touch = []
        #The following params are read in from a file with dict p.
        im = __import__(p_file)
        for k in im.p.keys():
            self.__setattr__(k,im.p[k])
            self._dont_touch.append(k)

#    def __setattr__(self,name,value):
#       """
#        
#        Overloading __setattr__, such that attributes cant be changed once they
#        are set, unless they are explicitely removed from the _dont_touch list.

#        """

#        if name == '_dont_touch':
#            super.__setattr__(self,name,value) 
#        elif name in self._dont_touch:
#            raise ValueError("Parameter %s is protected, please don't touch!"%name)
#        else:
#            super.__setattr__(self,name,value)
#            self._dont_touch.append(name)

    def set_by_gui(self, full=True):
        """
        Set additional parameters through a wx GUI object. The wx app needs to
        be started and set in the main loop

        Parameters
        ----------
        full : bool
            Whether we want the full gui (including cue reliability and so on)
            or the abridged GUI for the hemi-field experiment.
        
        """
        # Use the GetFromGui class (below):
        user_choice = GetFromGui(None, -1, 'Params', full=full)
        # success is achieved if the user presses 'done': 
        if user_choice.success:
            if full:
                user_params = {
                    "subject" : user_choice.subject,
                    "center_ori" : user_choice.center_ori,
                    "surr_ori" : user_choice.surr_ori,
                    "surround_w_target": user_choice.swt,
                    "audio_feedback": user_choice.audio,
                    "cue_reliability":user_choice.cue_reliability,
                    }
            else:
                user_params = {
                    "subject" : user_choice.subject,
                    "center_ori" : user_choice.center_ori,
                    "surr_ori" : user_choice.surr_ori,
                    }
        else:
            user_choice.Destroy()
            raise ValueError("Program stopped by user")
        # Stop execution of the window
        user_choice.Destroy()
        
        for k in user_params.keys():
            self.__setattr__(k,user_params[k])

    def save(self,f,open_and_close=False):
        """

        This saves the parameters to a text file.

        Takes as an input an already opened file object and returns it in the
        end. Does not open or close the file, unless the variable
        open_and_close is set to True, in which case, the input should be a
        file-name, not a file object

        f is returned either way
        """

        if open_and_close:
            f = file(file_name,'w')
            for k in self.__dict__.keys():
                if k[0]!='_': #Exclude 'private' variables ('_dont_touch')
                    f.write('# %s : %s \n'%(k,self.__dict__[k]))
            f.close()

        else:
            for k in self.__dict__.keys():
                if k[0]!='_': #Exclude 'private' variables ('_dont_touch')
                    f.write('# %s : %s \n'%(k,self.__dict__[k]))
        return f
    
def sound_freq_sweep(startFreq, endFreq, duration, samples_per_sec=None):
    """   
    Creates a normalized sound vector (duration seconds long) where the
    frequency sweeps from startFreq to endFreq (on a log2 scale).

    Parameters
    ----------

    startFreq: float, the starting frequency of the sweep in Hz
    
    endFreq: float, the ending frequency of the sweep in Hz

    duration: float, the duration of the sweep in seconds

    samples_per_sec: float, the sampling rate, defaults to 44100 


    """
    from psychopy.sound import Sound as Sound

    if samples_per_sec is None:
        samples_per_sec = 44100

    time = np.arange(0,duration*samples_per_sec)

    if startFreq != endFreq:
        startFreq = np.log2(startFreq)
        endFreq = np.log2(endFreq)
        freq = 2**np.arange(startFreq,endFreq,(endFreq-startFreq)/(len(time)))
        freq = freq[:time.shape[0]]
    else:
        freq = startFreq
    
    snd = np.sin(time*freq*(2*np.pi)/samples_per_sec)

    # window the sound vector with a 50 ms raised cosine
    numAtten = np.round(samples_per_sec*.05);
    # don't window if requested sound is too short
    if len(snd) >= numAtten:
        snd[:numAtten/2] *= window_hanning(np.ones(numAtten))[:numAtten/2]
        snd[-(numAtten/2):] *= window_hanning(np.ones(numAtten))[-(numAtten/2):]

    # normalize
    snd = snd/np.max(np.abs(snd))

    return snd

def compound_sound(freqs, duration, samples_per_sec=None):
    """
    Generate a sound made out of several frequencies

    Parameters
    ---------
    freqs: list
        A list of frequencies to be included in the 
    
    """
    if samples_per_sec is None:
        samples_per_sec = 44100

    time = np.arange(0,duration*samples_per_sec)
    snd = np.zeros_like(time)
    
    for f in freqs:
        snd =  snd + np.sin(time*f*(2*np.pi)/samples_per_sec)

    # window the sound vector with a 50 ms raised cosine
    numAtten = np.round(samples_per_sec*.05);
    # don't window if requested sound is too short
    if len(snd) >= numAtten:
        snd[:numAtten/2] *= window_hanning(np.ones(numAtten))[:numAtten/2]
        snd[-(numAtten/2):] *= window_hanning(np.ones(numAtten))[-(numAtten/2):]

    # normalize
    snd = snd/np.max(np.abs(snd))

    return snd
    
def start_data_file(subject_id, data_dir='./data'):

    """Start a file object into which you will write the data, while making
    sure not to over-write previously existing files """
    
    #Check the data_file:
    
    list_data_dir = os.listdir(data_dir)

    i=1
    this_data_file = '%s_%s_%s_att_ss.csv'%(subject_id,
                                                time.strftime('%m%d%Y'),i)

    #This makes sure that you don't over-write previous data:
    while this_data_file in list_data_dir:
        i += 1
        this_data_file='%s_%s_%s_att_ss.csv'%(subject_id,
                                                  time.strftime('%m%d%Y'),i)
        
    #Open the file for writing into:
    f = file('%s/%s'%(data_dir, this_data_file),'w')

    #Write some header information
    f.write('# Time : %s#\n'%(time.asctime()))

    return f

def save_data(f,*arg):

    for a in arg[0:-1]:
        f.write('%s,'%a)

    #Don't put a comma after the last one:
    f.write('%s \n'%arg[-1])
    
    return f

class Text(object):

    """
    A class for showing text on the screen until a key is pressed 
    """

    def __init__ (self,win,text='Press a key to continue',**kwargs):
        """
        
        Will do the default thing(show 'text' in white on gray background),
        unless you pass in kwargs, which will just go through to
        visual.TextStim (see docstring of that class for more details)

        keys: list. The keys to which you listen for input
        """

        self.win = win
        
        self.text = visual.TextStim(win,text=text,**kwargs)
        
    
    def __call__(self,duration=np.inf):
        """
        Text is shown to the screen, until a key is pressed or until duration
        elapses (default = inf)
        
        """

        clock = core.Clock()
        t=0
        while t<duration: #Keep going for the duration
            t=clock.getTime()

            self.text.draw()
            self.win.flip()

            for key in event.getKeys():
                if key:
                    return

def get_data(file_name):
    file_read = file(file_name,'r')
    l = file_read.readline()
    p = {} #This will hold the params
    l = file_read.readline()
    data_rec = []
    
    if l=='':
        return p,l,data_rec

    while l[0]=='#':
        try:
            p[l[1:l.find(':')-1]]=float(l[l.find(':')+1:l.find('\n')]) 

        #Not all the parameters can be cast as float (the task and the
        #subject): 
        except:
            p[l[2:l.find(':')-1]]=l[l.find(':')+1:l.find('\n')]

        l = file_read.readline()

    try:
        data_rec = csv2rec(file_name)
    except ValueError:
        p = []
    
    return p,l,data_rec



class Staircase(object):
    """
    This is an object for holding, updating and potentially analyzing
    A psychophysical staircase

    """ 
    def __init__(self,start,step,n_up=3,n_down=1,harder=-1,ub=1,lb=0):
        """
        Initialization function for the staircase class

        Parameters
        ----------
        start: The starting value of the staircase
        step: The size of the step used when updating the staircase
        n_up,n_down: The kind of staircase to be used, defaults to a 3-up,
                     1-down staircase 

        harder: {-1,1} The direction which would make the task harder. Defaults
        to -1, which is true for a contrast detection task.

        ub: float, the upper bound on possible values in the staircase
        lb: float, the lower bound on possible values in the staircase
        
        """
        
        self.value = start
        self.n_up = n_up
        self.step = step
        self.n = 0 #This is what will be compared to n_up for udpating.
        self.harder = np.sign(harder) #Make sure that this is only -1 or 1.
        self.record = [start]
        self.correct = []
        self.ub = ub
        self.lb = lb
        
    def update(self,correct):
        """

        This function updates the staircase value, according to the state of
        the staircase, the n/n_up values and whether or not the subject got it
        right. This staircase is then propagated on to the next trial.

        Parameters
        ----------
        correct: {True|False|None => don't update, but record the value} 

        """
        self.correct.append(correct)

        #If none is the input, don't change anything (not even n!) and record
        #the value in this trial:
        if correct is not None:
            if correct:
                if self.n>=self.n_up-1:
                    self.value += self.harder * self.step #'harder' sets the
                                                          #sign of the change
                                                          #to make it harder
                    self.n = 0
                else:
                    self.n +=1

            else:
                self.n = 0
                self.value -= self.harder * self.step #Change in the
                                            #opposite direction than above to
                                            #make it easier!
            #Make sure that the staircase doesn't     
            if self.value > self.ub:
                self.value = self.ub
            if self.value < self.lb:
                self.value = self.lb
            
        #Add to the records the updated value (even on trials where
        #correct=None):
        self.record.append(self.value)
        
    def analyze(self, guess=0.5, flake=0.01, slope=3.5, fig_name=None,
                bootstrap_n=1000):
        """
        Perform a psychometric curve analysis of the data in the staircase and
        save a figure, if needed.

        Parameters
        ----------

        guess: The expected hit rate when the subject is blind-folded (default:
        0.5)

        flake: The expected rate of misses on trials on which the subjects
        should actually succeed, if they are really doing the task (default: 0.1)

        slope: The slope of the psychometric curve at the inflection point
        (default to 3.5)

        fig_name: string
           A file name for saving a figure. If none provided, don't save the
           generated figure

        bootstrap_n: int
           The number of boot samples to take for the bootstrapping analysis
           
        Note
        ----

        The fitting procedure is applied to the slope, as well as to the
        threshold.
        
        """
        def weibull(x,threshx,slope,guess,flake,threshy=None):
                if threshy is None:
                    threshy = 1-(1-guess)*np.exp(-1)

                k = (-np.log( (1-threshy)/(1-guess) ))**(1/slope)
                weib = flake - (flake-guess)*np.exp(-(k*x/threshx)**slope)
                return weib 

        def get_thresh(amp,c):
            """Calculate a threshold given amp, c(orrect) values  """ 
            #Helper functions for fitting the psychometric curve, need to be
            #defined within the local scope, so that they can grok the data:
            
            def weib_fit(pars):
                thresh,slope = pars
                return weibull(x,thresh,slope,guess,flake)

            def err_func(pars):
                return y-weib_fit(pars)

            #Throw away the None's:
            hit_amps = amp[c==1]
            miss_amps = amp[c==0]

            # Get rid of floating point error:
            hit_amps = defloaterrorize(hit_amps)
            miss_amps = defloaterrorize(miss_amps)

            all_amps = np.hstack([hit_amps,miss_amps])
            stim_intensities = np.unique(all_amps)

            n_correct = [len(np.where(hit_amps==i)[0]) for i in stim_intensities]
            n_trials = [len(np.where(all_amps==i)[0]) for i in stim_intensities]
            Data = zip(stim_intensities,n_correct,n_trials)
            x = []
            y = []
            n = []
            for idx,this in enumerate(Data):
                #Take only cases where there were at least n_up observations:
                if n_trials[idx]>=self.n_up:
                    #Contrast values: 
                    x = np.hstack([x,this[2] * [this[0]]])
                    #% correct:
                    y = np.hstack([y,this[2] * [this[1]/float(this[2])]])

            initial = np.mean(x),slope
            this_fit , msg = leastsq(err_func,initial)
            return this_fit,x,y
        
        #Convert the flake into the expected format for the weibull function:
        flake = 1-flake
        amp = np.array(self.record[:-1]) #The last one will be the next trials
                                        #amp 
        c = np.array(self.correct) #Which is why correct is one item shorter
        
        this_fit,keep_x,keep_y = get_thresh(amp,c)
        #print keep_x
        #print keep_y
        
        bootstrap_th = []
        bootstrap_slope = []
        keep_amp = amp
        keep_c = c
        keep_slope = this_fit[1]
        keep_th = this_fit[0]
        for b in xrange(bootstrap_n):
            b_idx = np.random.randint(0,c.shape[0],c.shape[0])
            amp = keep_amp[b_idx]
            c = keep_c[b_idx]
            this_fit,x,y = get_thresh(amp,c)
            bootstrap_th.append(this_fit[0])

        upper = np.sort(bootstrap_th)[bootstrap_n*0.84]
        lower = np.sort(bootstrap_th)[bootstrap_n*0.16]

        #Make a figure, if required:
        if fig_name is not None: 
            fig = plt.figure()
            ax = fig.add_subplot(1,1,1)
            for idx,this_x in enumerate(keep_x):
                n = np.sum(keep_x==this_x)  # How many trials, sets the markersize
                ax.plot(this_x,keep_y[idx],'o',color = 'b',markersize = n)

            x_for_plot = np.linspace(np.min(keep_x)-0.05,np.max(keep_x)+0.05,100)
            ax.plot(x_for_plot,weibull(x_for_plot,keep_th,
                                       keep_slope,
                                       guess,
                                       flake),
                    color = 'g')
            ax.set_title('Threshold=%1.2f +/- %1.2f ::Slope=%1.2f'
                         %(keep_th,(upper-lower)/2,keep_slope))
            fig.savefig(fig_name)

        return keep_th,lower,upper

# Helper function in order to get rid of small round-off error in the
# representation of trial contrasts in the staircase object:
def defloaterrorize(a):
    # Turn into units of % contrast: 
    a *= 100
    # Truncate anything smaller than 1% contrast:
    a = a.astype(int)
    # Recover the original units (0-1):
    a = a/100.0
    return a

def get_data(file_name=None, even_or_odd=False):
    """
    Get the data from file, returning parameters, the line with variable names
    and a data_rec using these variable names.

    Parameters
    ----------

    file_name : str
          Full path to the file to be analyzed. If None, a gui opens to let
          users choose the file in their file-system.
          
    even_or_odd : str
       For split half analysis, this variable allows choosing to output only
       even or only odd trials from the file
    
    """

    if file_name is None: 
        path_to_files = './data/'
        file_name =  str(gui.fileOpenDlg(path_to_files)[0])
    
    file_read = file(file_name,'r')
    l = file_read.readline()
    p = {} #This will hold the params
    l = file_read.readline()
    data_rec = []
    
    if l=='':
        return p,l,data_rec

    while l[0]=='#':
        try:
            p[l[1:l.find(':')-1]]=float(l[l.find(':')+1:l.find('\n')]) 
        except:
            p[l[2:l.find(':')-1]]=l[l.find(':')+1:l.find('\n')]
        l = file_read.readline()
        
    try:
        data_rec = csv2rec(file_name)
    except ValueError:
        p = []

    # For split half analysis, you might want to pull only even or only odd
    # trials (and compare...):
    if even_or_odd == 'odd':
        data_rec = data_rec[::2] 
    elif even_or_odd == 'even':
        data_rec = data_rec[1::2]
            
    return p,l,data_rec


def cumgauss(x, mu, sigma, low_asym=0, high_asym=1):
    """
    The cumulative Gaussian at x, for the distribution with mean mu and
    standard deviation sigma. Additional parameters allow fitting high and
    low asymptote if needed

    Based on:
    http://en.wikipedia.org/wiki/Normal_distribution#Cumulative_distribution_function

    """
    cg = 0.5 * (1 + erf((x-mu)/(np.sqrt(2)*sigma)))
    #cg = cg/np.max(cg) * (high_asym - low_asym)
    #cg = cg + low_asym
    return cg 

def weibull(x, threshx, slope, guess, flake, threshy=None):
    """
    The cumulative Weibull distribution function at x.
    """
    if threshy is None:
        threshy = 1-(1-guess)*np.exp(-1)

    k = (-np.log( (1-threshy)/(1-guess) ))**(1/slope)
    weib = flake - (flake-guess)*np.exp(-(k*x/threshx)**slope)
    return weib 



def fit_th(x, y, initial, fit_func='cumgauss'):
    """

    The core of the fitting. Get x values and get the responses (between 0
    and 1). Then, fit the function according to these values
    
    """
    def cumgauss_fit(params, x):
        """
        fit func
        """
        mu,sigma = params
        return cumgauss(x, mu, sigma)


    def weib_fit(params, x):
        """
        Fitting function for fitting a weibull function
        """
        thresh, slope, guess, flake = params
        return weibull(x, thresh, slope, guess, flake)


    def cumgauss_fit_w_asym(params, x):
        """
        Fitting function for cumulative Gaussian with asymptotes
        """
        mu,sigma,low_a,high_a = params
        return cumgauss(x, mu, sigma, low_a, high_a)

    def err_func(params, x, y, fit_func):
        """
        Error function. Handles boundary setting. For example, thresholds can't
        be smaller than 0, or larger than 1.

        """
        if fit_func=='cumgauss':
            if params[0] > 1:
                return np.inf
            if params[0] < 0:
                return np.inf
            return y - cumgauss_fit(params, x)
        elif fit_func=='cumgauss_w_asym':
            if params[2]<0:
                return np.inf
            if params[3]>1:
                return np.inf
            return y - cumgauss_fit_w_asym(params, x)
        elif fit_func=='weib':
            if params[0] > 1:
                return np.inf
            if params[0] < 0:
                return np.inf
            #if params[1] < 3.4:
            #    return np.inf
            #if params[1] > 3.6:
            #    return np.inf
            return y - weib_fit(params, x)

    this_fit, msg = leastsq(err_func, initial, args=(x, y, fit_func))
    # If you get back a nan, replace with the initial guess:
    this_fit[np.isnan(this_fit)] = np.array(initial)[np.isnan(this_fit)]
    return this_fit

def analyze_constant(data_file=None, fig_name=None, cue_cond='cued',
                     fit_func='cumgauss', log_scale=False, boot=1000,
                     leave_one_out=False, verbose=True, even_or_odd=False,
                     clamp_to_0_1=True):
    """
    This analyzes data from the constant stimuli experiment

    Parameters
    ----------
    data_file : str,
        the full path to a file with some data. If this is not provided, the
        user will be prompted to navigate to the file with a file-browser. 

    fig_name : str, 
        The name of the figure to save. If not provided, the figure will not be
        saved.
    cue_cond : str,
        whether to analyze the 'cued' condition or the 'other' condition.

    fit_func : str,
         What type of function to fit to the data ('cumgauss' or 'weib')

    log_scale : bool,
        Whether to transform things into a log scale (FWIW).

    boot : int,
        The number of iterations of fitting in the boot-strap procedure.

    leave_one_out : bool
        Whether to 

    even_or_odd : bool
        Whether to take only even or only odd trials in each sample.
    """

    p,l,data_rec = get_data(data_file, even_or_odd=even_or_odd)

    if cue_cond == 'cued':
        cue_cond_idx = np.where(data_rec['cue_side']==data_rec['ask_side'])[0]
    elif cue_cond == 'other':
        cue_cond_idx = np.where(data_rec['cue_side']!=data_rec['ask_side'])[0]

    center_contrasts = [float(t) for t in
    p['center_contrast'].split('[')[1].split(']')[0].split(' ') if t !='' ]
    
    base_contrast = []
    for i,ask_side in enumerate(data_rec['ask_side'][cue_cond_idx]):
        base_contrast.append(data_rec[ask_side + '_contrast2'][cue_cond_idx][i])

    if fig_name is not None: 
        # Get the color cycle from the mpl rc params:
        rc = matplotlib.rc_params()
        colors = rc['axes.color_cycle'][:len(center_contrasts)]
        fig = plt.figure()
        ax = fig.add_subplot(1,1,1)

    fits = []
    keep_x = []
    keep_y = []

    boot_th_ub = []
    boot_sl_ub = []
    boot_th_lb = []
    boot_sl_lb = []

    min_x=1
    max_x=0
    for contrast in center_contrasts:
        c_idx = np.where(np.abs(np.array(base_contrast)-contrast)<0.01)[0]
        this_ask = data_rec['ask_contrast'][cue_cond_idx][c_idx]
        # Move it into the interval [0,1] with the right directionality: when
        # the value of this_ask was high, the chances were higher for a '1'
        # answer than for a '2' answer (that's the "1 - " at the beginning of
        # next line):
        this_ans = 1 - (data_rec['answer'][cue_cond_idx][c_idx] - 1) 
        x = np.array(contrast) + this_ask

        if log_scale:
            x = np.log10(x)
        if clamp_to_0_1:
            x[np.where(x>1)] = 1
            x[np.where(x<0)] = 0
        
        # Begin by guessing that the mean is the same as the contrast shown
        # (no bias):
        if fit_func == 'cumgauss':
            initial = contrast, 1
        elif fit_func == 'cumgauss_w_asym':
            initial = contrast, 1, 0, 1
        elif fit_func == 'weib':
            initial = contrast, 3.5, 0, 1

        # Generate the y axis:
        y = []
        for i in range(len(this_ans)):
            y.append(np.mean(this_ans[this_ask==this_ask[i]]))

        if verbose:
            print("Using the %s function to analyze this"%fit_func)

        if even_or_odd == 'even':
            fit_x = x[::2]
            fit_y = y[::2]
        elif even_or_odd == 'odd':
            fit_x = x[1::2]
            fit_y = y[1::2]
        else:
            fit_x = x
            fit_y = y

        this_fit = fit_th(fit_x, fit_y, initial, fit_func)
        # Store stuff for plotting:
        fits.append(this_fit)
        keep_x.append(x)
        keep_y.append(y)

        min_x = min([min_x, np.min(x)])
        max_x = max([max_x, np.max(x)])
        boot_th = []
        boot_sl = []
        # Bootstrap estimate the parameters
        for b in range(boot):
            # Choose this boot sample
            idx = np.random.randint(0, len(x), len(x))
            boot_x = x[idx]
            boot_ans = this_ans[idx]
            boot_ask = this_ask[idx]
            boot_y = []
            for i in range(len(boot_ans)):
                boot_y.append(np.mean(boot_ans[boot_ask==boot_ask[i]]))

            # Use the same initial value guess as above:
            boot_fit = fit_th(boot_x, boot_y, initial, fit_func)
            boot_th.append(boot_fit[0])
            boot_sl.append(boot_fit[1])
            
        sort_th = np.sort(boot_th)
        sort_sl = np.sort(boot_sl)

        boot_th_ub.append(sort_th[0.84*boot])
        boot_th_lb.append(sort_th[0.16*boot])
        boot_sl_ub.append(sort_sl[0.84*boot])
        boot_sl_lb.append(sort_sl[0.16*boot])

    if fig_name is not None:
        for i,fit in enumerate(fits):
            for idx, this_x in enumerate(keep_x[i]):
                n = np.sum(keep_x[i]==this_x) # How many trials, sets the
                                               # markersize
                ax.plot(this_x,keep_y[i][idx],
                        'o',color = colors[i], markersize = n)
                
                x_for_plot = np.linspace(min_x-0.05,
                                         max_x+0.05,100)

            if fit_func == 'cumgauss':
                plotter = cumgauss
            elif fit_func == 'cumgauss_w_asym':
                plotter = cumgauss
            elif fit_func == 'weib':
                plotter = weibull
            ax.plot(x_for_plot,plotter(x_for_plot,*fits[i]),
                    color = colors[i])
            texter = (
                'PSE: %1.2f +/- %1.2f \nslope: %1.2f +/- %1.2f'%(fits[i][0],
                                            (boot_th_ub[i]-boot_th_lb[i])/2,
                                                      fits[i][1],
                                            (boot_sl_ub[i]-boot_sl_lb[i])/2))
            print("For the file: %s \n %s \n"%(data_file, cue_cond) + texter)

            # Indicate the values of the fit:
            ax.text(fits[i][0] + 0.1, fits[i][0] + 0.1,texter)

        fig.savefig(fig_name)

    out = dict(x=[], y=[], trials=[],
               fit=[],
               boot_th_lb=boot_th_lb, boot_th_ub=boot_th_ub,
               boot_sl_lb=boot_sl_lb, boot_sl_ub=boot_sl_ub)

    # Make the return values:
    for i, fit in enumerate(fits):
        out['fit'].append(fit)
        for idx, this_x in enumerate(np.unique(keep_x[i])):
            x_idx = np.where(keep_x[i]==this_x)[0]
            out['x'].append(x[x_idx[0]])
            out['y'].append(y[x_idx[0]])
            out['trials'].append(np.sum(keep_x[i]==this_x))
    return out

def get_df(n_subjects,
	   path_to_files='/Users/arokem/Dropbox/att_ss/Analysis/',
	   file4R='/Users/arokem/Dropbox/att_ss/file4R.csv',
	   fit_func='cumgauss',
	   boots=1,
	   exclude=None,
	   verbose=True,
	   cue_conds=['cued', 'other', 'neutral'],
           even_or_odd=False):

    """

    Make pandas data-frames from a whole data-set.

    df contains all of the data (including performance on individual contrast
    levels and so forth), but is very complicated.

    df2 contains a summary of the data, but is very simple 
    
    """
    n_params_dict = dict(cumgauss=2,
                         cumgauss_w_asym=4,
                         weib=4)
    
    n_params = n_params_dict[fit_func]
    dirlist = os.listdir(path_to_files)

    sub_id = ['S%02d'%(i+1) for i in range(n_subjects)]

    if exclude is not None:
        # Make sure to exclude from the largest to smallest:
        exclude = np.sort(exclude)[::-1]
        for ex in exclude:
            sub_id = sub_id[:ex-1] + sub_id[ex:]

    if verbose:
            print sub_id


    df = {}
    df2 = {'subject':[],
           'abs_ori':[],
           'rel_ori':[],
           'cue':[]}

    for para in range(n_params):
            df2.update({'p%i'%(para+1):[]})
    
    surr_k = ' surr_ori'
    center_k = ' center_ori'
    for this_sub in sub_id:
        if verbose:
                print("Analyzing %s"%this_sub)
        df[this_sub] = {(0,0):{}, (0,90):{}, (90,0):{}, (90,90):{}}
        for this_file in dirlist:
            # Only look at files from this subject:
            if this_file.startswith(this_sub):
                if verbose:
                        print("File: %s"%this_file)
                p,l,d = get_data(path_to_files + this_file)
                # The key differs, depending on the 
                if p.has_key('cue_reliability'):
                    cue_reliability = p['cue_reliability']
                else:
                    cue_reliability = p[' cue_reliability']
                if isinstance(cue_reliability, float):
                    conds = cue_conds[:2]
                    for cue in conds:
                        if verbose: 
                                print("Condition: %s"%cue)
                        this=analyze_constant(path_to_files+this_file,
                                              cue_cond=cue,
                                              log_scale=False,
                                              fit_func=fit_func,
                                              boot=boots,
                                              verbose=verbose,
                                              even_or_odd=even_or_odd)

                        for ii in range(len(this['fit'][0])):
                            if this['fit'][0][ii] > 1.0:
                                print this['fit'][0][ii]
                                this['fit'][0][ii] = 1.0
                                print this['fit'][0][ii]
                                
                        df[this_sub][p[center_k],p[surr_k]][cue]=this
                        df2['subject'].append(this_sub)
                        df2['abs_ori'].append(str(p[center_k]))
                        df2['rel_ori'].append(str(np.abs(p[center_k] -
                                                         p[surr_k])))
                        df2['cue'].append(cue)

                        for idx, para in enumerate(this['fit'][0]):
                            df2['p%i'%(idx+1)].append(para)

                else:
                    if verbose:
                            print("Condition: neutral")

                    # The neutral condition takes "other" as input:
                    this = analyze_constant(path_to_files + this_file,
                                                  cue_cond='other',
                                                  log_scale=False,
                                                  fit_func=fit_func,
                                                  boot=boots,
                                                  verbose=verbose)

                    df[this_sub][p[center_k],p[surr_k]]['neutral']=this
                    df2['subject'].append(this_sub)
                    # These should be treated as categorical in the ANOVA:
                    df2['abs_ori'].append(str(p[center_k]))
                    df2['rel_ori'].append(str(np.abs(p[center_k] - p[surr_k])))
                    df2['cue'].append(cue)

                    for idx, para in enumerate(this['fit'][0]):
                            df2['p%i'%(idx+1)].append(para)

    return pd.DataFrame(df), pd.DataFrame(df2)

def save_spss_files(df, path='/Users/arokem/Dropbox/att_ss'):
    """
    Record stuff from the complicated df into files in an spss format
    """ 
    file_out_th = file(path + '/file4SPSS_th.csv', 'w')
    file_out_sl = file(path + '/file4SPSS_sl.csv', 'w')

    cue_conds = df[df.columns[0]][1].keys()
    
    # Make the header row:
    for file_out in [file_out_th, file_out_sl]:
        file_out.write('subject, ' + ''.join(['%s_%s_%s, '%(i,j,k)
                                     for i in cue_conds
                                     for j in [0,90]
                                     for k in [0,90]]) + '\n')

    for sub in df.columns:
        file_out_th.write('%s, '%sub +
                       ''.join(['%s, '%df[sub][i,j][cond]['fit'][0][0]
                                for cond in cue_conds
                                for i in [0,90]
                                for j in [0,90]
                                ]) + '\n')

        file_out_sl.write('%s, '%sub +
                       ''.join(['%s, '%df[sub][i,j][cond]['fit'][0][1]
                                for cond in cue_conds
                                for i in [0,90]
                                for j in [0,90]
                                ]) + '\n')

    file_out_th.close()
    file_out_sl.close()


def coeff_of_determination(data, model, axis=-1):
    """

     http://en.wikipedia.org/wiki/Coefficient_of_determination

              _                                            _
             |    sum of the squared residuals              |
    R^2 =    |1 - ---------------------------------------   | * 100
             |_    sum of the squared mean-subtracted data _|


    """
    # There's no point in doing any of this: 
    if np.all(data==0.0) and np.all(model==0.0):
        return np.nan
    
    residuals = data - model
    ss_err = np.sum(residuals ** 2, axis=axis)

    demeaned_data = data - np.mean(data,-1)[...,np.newaxis]
    ss_tot = np.sum(demeaned_data **2, axis=axis)

    # Don't divide by 0:
    if np.all(ss_tot==0.0):
        return np.nan
    
    return 1 - (ss_err/ss_tot)

def split_half(df, cue, center_ori, surr_ori, perm=None):
    """

    Get y from one (randomly chosen) half of the data and params from the other
    half.
    
    """
    keys = df.keys()
    if perm is None:
        perm = np.random.permutation(keys)
    y_keys = perm[:len(keys)/2]
    param_keys = perm[len(keys)/2:]

    y = []
    for k in y_keys:
        y.append(df[k][surr_ori, center_ori][cue]['y'])
        x = df[k][surr_ori, center_ori][cue]['x']

    mu = []
    sigma = []
    for k in param_keys:
        params = df[k][surr_ori,center_ori][cue]['fit'][0]
        mu.append(params[0])
        sigma.append(params[1])
        
    return np.array(x), np.array(y), np.array(mu), np.array(sigma)


def model_evaluation_split_half(df):
    """
    Evaluate models with split half cross-validation
    """    
    oris=[0,90]
    cue_conds = ['cued', 'other', 'neutral']


    sh_y = {(0,0):dict.fromkeys(cue_conds),
             (0,90):dict.fromkeys(cue_conds),
             (90,0):dict.fromkeys(cue_conds),
             (90,90):dict.fromkeys(cue_conds)}

    sh_mu = {(0,0):dict.fromkeys(cue_conds),
             (0,90):dict.fromkeys(cue_conds),
             (90,0):dict.fromkeys(cue_conds),
             (90,90):dict.fromkeys(cue_conds)}

    sh_sigma = {(0,0):dict.fromkeys(cue_conds),
                 (0,90):dict.fromkeys(cue_conds),
                 (90,0):dict.fromkeys(cue_conds),
                 (90,90):dict.fromkeys(cue_conds)}

    for center_ori in oris:
        for surr_ori in oris: 
            for cue in cue_conds:
                perm = np.random.permutation(df.keys())
                (x,
                 sh_y[center_ori, surr_ori][cue],
                 sh_mu[center_ori, surr_ori][cue],
                 sh_sigma[center_ori, surr_ori][cue]) = split_half(df,
                                                                   cue,
                                                                   center_ori,
                                                                   surr_ori,
                                                                   perm)

    RR = []

    # First model: nothing matters: all averages all the time:
    y = []
    pred = []
    for center_ori in oris:
        for surr_ori in oris: 
            for cue in cue_conds:
                y.append(np.mean(sh_y[center_ori, surr_ori][cue], 0))
                
                mu = np.mean([np.mean(sh_mu[0, 0]['cued'],0),
                              np.mean(sh_mu[0, 0]['neutral'],0),
                              np.mean(sh_mu[0, 0]['other'],0),
                              np.mean(sh_mu[0, 90]['cued'],0),
                              np.mean(sh_mu[0, 90]['neutral'],0),
                              np.mean(sh_mu[0, 90]['other'],0),
                              np.mean(sh_mu[90, 90]['cued'],0),
                              np.mean(sh_mu[90, 90]['neutral'],0),
                              np.mean(sh_mu[90, 90]['other'],0),
                              np.mean(sh_mu[90, 0]['cued'],0),
                              np.mean(sh_mu[90, 0]['neutral'],0),
                              np.mean(sh_mu[90, 0]['other'],0)])

                sigma = np.mean([np.mean(sh_sigma[0, 0]['cued'],0),
                              np.mean(sh_sigma[0, 0]['neutral'],0),
                              np.mean(sh_sigma[0, 0]['other'],0),
                              np.mean(sh_sigma[0, 90]['cued'],0),
                              np.mean(sh_sigma[0, 90]['neutral'],0),
                              np.mean(sh_sigma[0, 90]['other'],0),
                              np.mean(sh_sigma[90, 90]['cued'],0),
                              np.mean(sh_sigma[90, 90]['neutral'],0),
                              np.mean(sh_sigma[90, 90]['other'],0),
                              np.mean(sh_sigma[90, 0]['cued'],0),
                              np.mean(sh_sigma[90, 0]['neutral'],0),
                              np.mean(sh_sigma[90, 0]['other'],0)])
                
                pred.append(cumgauss(x, mu, sigma))
    RR.append(coeff_of_determination(np.array(y).ravel(),np.array(pred).ravel()))

    # Second model: both mu and sigma depend on both surround condition
    # (para/ortho) and attention (cued/other/neutral):  
    y = []
    pred = []
    for center_ori in oris:
        for surr_ori in oris: 
            for cue in cue_conds:
                y.append(np.mean(sh_y[center_ori, surr_ori][cue], 0))
                if center_ori == surr_ori:
                    sigma = np.mean([np.mean(sh_sigma[0, 0][cue],0),
                                     np.mean(sh_sigma[90, 90][cue],0)])
                                    
                    mu = np.mean([np.mean(sh_mu[0, 0][cue],0),
                                  np.mean(sh_mu[90, 90][cue],0)])
                                    
                else:
                    sigma = np.mean([np.mean(sh_sigma[0, 90][cue],0),
                                     np.mean(sh_sigma[90, 0][cue],0)])

                    mu = np.mean([np.mean(sh_mu[0, 90][cue],0),
                                  np.mean(sh_mu[90, 0][cue],0)])
     
                pred.append(cumgauss(x, mu, sigma))
    RR.append(coeff_of_determination(np.array(y).ravel(),np.array(pred).ravel()))

    
    # Third model: mu depends on surround condition (para/ortho), sigma
    # depends on both surround condition (para/ortho) and attention
    # (cued/other/neutral):  
    y = []
    pred = []
    for center_ori in oris:
        for surr_ori in oris: 
            for cue in cue_conds:
                y.append(np.mean(sh_y[center_ori, surr_ori][cue], 0))
                if center_ori == surr_ori:
                    sigma = np.mean([np.mean(sh_sigma[0, 0][cue],0),
                                     np.mean(sh_sigma[90, 90][cue],0)])
                                    
                    mu = np.mean([np.mean(sh_mu[0, 0]['cued'],0),
                                  np.mean(sh_mu[0, 0]['neutral'],0),
                                  np.mean(sh_mu[0, 0]['other'],0),
                                  np.mean(sh_mu[90, 90]['cued'],0),
                                  np.mean(sh_mu[90, 90]['neutral'],0),
                                  np.mean(sh_mu[90, 90]['other'],0)])
                else:
                    sigma = np.mean([np.mean(sh_sigma[0, 90][cue],0),
                                     np.mean(sh_sigma[90, 0][cue],0)])

                    mu = np.mean([np.mean(sh_mu[0, 90]['cued'],0),
                                  np.mean(sh_mu[0, 90]['neutral'],0),
                                  np.mean(sh_mu[0, 90]['other'],0),
                                  np.mean(sh_mu[90, 0]['cued'],0),
                                  np.mean(sh_mu[90, 0]['neutral'],0),
                                  np.mean(sh_mu[90, 0]['other'],0)])

                              
                pred.append(cumgauss(x, mu, sigma))
    RR.append(coeff_of_determination(np.array(y).ravel(),np.array(pred).ravel()))

    return RR

def loo_data(df, cue, center_ori, surr_ori):
    """
    Get leave-one-out data for a particular condition
    """
    mu = []
    sigma = []
    loo_y = []
    keys = df.keys()
    for loo in keys:
        this_loo = df[loo]
        loo_y.append(this_loo[surr_ori, center_ori][cue]['y'])
        x = this_loo[surr_ori, center_ori][cue]['x']
        mu.append([])
        sigma.append([])
        
        for k  in keys:
            if k!=loo:
                this_sub = df[k]
                params = this_sub[surr_ori,center_ori][cue]['fit'][0]
                mu[-1].append(params[0])
                sigma[-1].append(params[1])
        
    return np.array(x), np.array(loo_y), np.array(mu), np.array(sigma)

def model_evaluation_loo(df):
    """

    Evaluate a model with LOO cross-validation

    """
    oris=[0,90]
    cue_conds = ['cued', 'other', 'neutral']
    loo_y = {(0,0):dict.fromkeys(cue_conds),
             (0,90):dict.fromkeys(cue_conds),
             (90,0):dict.fromkeys(cue_conds),
             (90,90):dict.fromkeys(cue_conds)}

    loo_mu = {(0,0):dict.fromkeys(cue_conds),
             (0,90):dict.fromkeys(cue_conds),
             (90,0):dict.fromkeys(cue_conds),
             (90,90):dict.fromkeys(cue_conds)}

    loo_sigma = {(0,0):dict.fromkeys(cue_conds),
                 (0,90):dict.fromkeys(cue_conds),
                 (90,0):dict.fromkeys(cue_conds),
                 (90,90):dict.fromkeys(cue_conds)}

    RR1 = {(0,0):dict.fromkeys(cue_conds),
           (0,90):dict.fromkeys(cue_conds),
           (90,0):dict.fromkeys(cue_conds),
           (90,90):dict.fromkeys(cue_conds)}

    RR2 = {(0,0):dict.fromkeys(cue_conds),
           (0,90):dict.fromkeys(cue_conds),
           (90,0):dict.fromkeys(cue_conds),
           (90,90):dict.fromkeys(cue_conds)}

    RR3 = {(0,0):dict.fromkeys(cue_conds),
           (0,90):dict.fromkeys(cue_conds),
           (90,0):dict.fromkeys(cue_conds),
           (90,90):dict.fromkeys(cue_conds)}

    for center_ori in oris:
        for surr_ori in oris: 
            for cue in cue_conds:
                (x,
                 loo_y[center_ori, surr_ori][cue],
                 loo_mu[center_ori, surr_ori][cue],
                 loo_sigma[center_ori, surr_ori][cue]) = loo_data(df,
                                                                  cue,
                                                                  center_ori,
                                                                  surr_ori)
                
    for center_ori in oris:
        for surr_ori in oris: 
            for cue in cue_conds:
                # First model: nothing matters, everything is just the mean:
                RR1[center_ori, surr_ori][cue]=[]
                for loo_idx in range(len(df.keys())):
                    mu = np.mean([loo_mu[0,0]['neutral'][loo_idx],
                                  loo_mu[0,90]['neutral'][loo_idx],
                                  loo_mu[90,90]['neutral'][loo_idx],
                                  loo_mu[90,0]['neutral'][loo_idx],
                                  loo_mu[0,0]['cued'][loo_idx],
                                  loo_mu[0,90]['cued'][loo_idx],
                                  loo_mu[90,90]['cued'][loo_idx],
                                  loo_mu[90,0]['cued'][loo_idx],
                                  loo_mu[0,0]['other'][loo_idx],
                                  loo_mu[0,90]['other'][loo_idx],
                                  loo_mu[90,90]['other'][loo_idx],
                                  loo_mu[90,0]['other'][loo_idx]])

                    sigma = np.mean([loo_sigma[0,0]['neutral'][loo_idx],
                                  loo_sigma[0,90]['neutral'][loo_idx],
                                  loo_sigma[90,90]['neutral'][loo_idx],
                                  loo_sigma[90,0]['neutral'][loo_idx],
                                  loo_sigma[0,0]['cued'][loo_idx],
                                  loo_sigma[0,90]['cued'][loo_idx],
                                  loo_sigma[90,90]['cued'][loo_idx],
                                  loo_sigma[90,0]['cued'][loo_idx],
                                  loo_sigma[0,0]['other'][loo_idx],
                                  loo_sigma[0,90]['other'][loo_idx],
                                  loo_sigma[90,90]['other'][loo_idx],
                                  loo_sigma[90,0]['other'][loo_idx]])

                    RR1[center_ori, surr_ori][cue].append(
            coeff_of_determination(loo_y[center_ori, surr_ori][cue][loo_idx],
                                   cumgauss(x, mu, sigma)))
                # Second model: just the specific condition:
                RR2[center_ori, surr_ori][cue]=[]
                for loo_idx in range(len(df.keys())):
                    mu=np.mean(loo_mu[center_ori, surr_ori][cue][loo_idx])
                    sigma=np.mean(loo_sigma[center_ori, surr_ori][cue][loo_idx])
                    
                    RR2[center_ori, surr_ori][cue].append(
            coeff_of_determination(loo_y[center_ori, surr_ori][cue][loo_idx],
                                           cumgauss(x, mu, sigma)))
                # Third model: surround suppression affects both, cueing
                # affects slope
                RR3[center_ori, surr_ori][cue]=[]
                for loo_idx in range(len(df.keys())):
                    mu=np.mean(np.hstack(
                        [loo_mu[center_ori, surr_ori]['cued'][loo_idx],
                         loo_mu[center_ori, surr_ori]['neutral'][loo_idx],
                         loo_mu[center_ori, surr_ori]['other'][loo_idx]]))
                    
                    sigma=np.mean(loo_sigma[center_ori,surr_ori][cue][loo_idx])
    
                    RR3[center_ori, surr_ori][cue].append(
            coeff_of_determination(loo_y[center_ori, surr_ori][cue][loo_idx],
                                   cumgauss(x, mu, sigma)))

    
    return RR1,RR2,RR3

def bootstrap_mean(x, alpha=0.05, b=1000):
    """
    Calculate bootstrap 1-alpha percentile CI of the mean from a sample x

    Parameters
    ----------
    x : 1d array

    alpha : float
      Confidence interval is defined as the 

    b : int
       The number of bootstrap samples

    Returns
    -------
    lb, ub : the lower and upper bounds of the confidence interval
    
    """
    means = np.empty(b)
    
    for ii in xrange(b):
        idx = np.random.randint(0, len(x), len(x))
        means[ii] = np.mean(x[idx])

    sort_means = np.sort(means)
    lb_idx = int(b * alpha/2)
    ub_idx = int(b * (1-(alpha/2)))

    return sort_means[lb_idx], sort_means[ub_idx]
    
        
        


    



    
